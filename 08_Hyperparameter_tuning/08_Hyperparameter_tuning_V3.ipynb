{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyperparameter tuning of a model consisting of linear layers with optuna.\n",
    "\n",
    " Install the python packages:\n",
    " - optuna\n",
    " - optuna-dashboard\n",
    "\n",
    "# See https://optuna.org/ for details about Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the below functionality to execute your model (that you will adjust later step by step)\n",
    "# This block of code provides you the functionality to train a model. Results are printed after each epoch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset into your directory.\n",
    "    You can change the root_path to point to a already existing path if you want to safe a little bit of memory :)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_fn, dataloader):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = []\n",
    "    for imgs, targets in dataloader:\n",
    "        imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "        max_outputs = torch.max(outputs, dim=1).indices\n",
    "        accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "        running_accuracy.append(accuracy)\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = torch.tensor(running_accuracy).mean()\n",
    "    # print(f'Training iteration finished with loss: {avg_loss:.3f} and accuracy {avg_acc:.3f}')\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def eval_model(model, loss_fn, dataloader):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in dataloader:\n",
    "            imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = torch.tensor(running_accuracy).mean()\n",
    "    # print(f'Evaluation iteration finished with loss: {avg_loss:.3f} and accuracy {avg_acc:.3f}')\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def operate(model, optimizer, loss_fn, train_dataloader, test_dataloader, epochs):\n",
    "    t_losses, t_accs = [], []\n",
    "    e_losses, e_accs = [], []\n",
    "    for epoch in range(0, epochs):\n",
    "        t_avg_loss, t_avg_acc = train_model(\n",
    "            model, optimizer, loss_fn, train_dataloader\n",
    "        )\n",
    "        t_losses.append(t_avg_loss)\n",
    "        t_accs.append(t_accs)\n",
    "\n",
    "        e_avg_loss, e_avg_acc = eval_model(\n",
    "            model, loss_fn, test_dataloader\n",
    "        )\n",
    "        e_losses.append(e_avg_loss)\n",
    "        e_accs.append(e_accs)\n",
    "\n",
    "    return torch.as_tensor(t_losses), torch.as_tensor(t_accs), torch.as_tensor(e_losses), torch.as_tensor(e_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two parameters to create your model\n",
    "# 1) the amount of hidden layers\n",
    "# 2) the neurons per hidden layer\n",
    "\n",
    "# we tune those two parameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "def build_model(trial):\n",
    "    layers = []\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_neurons_per_layer = trial.suggest_int(\"n_neurons\", 8, 64)\n",
    "\n",
    "    first_layer = Linear(784, n_neurons_per_layer)\n",
    "    layers.append(first_layer)\n",
    "\n",
    "    for i in range(n_layers - 1):\n",
    "        layers.append(Linear(n_neurons_per_layer, n_neurons_per_layer))\n",
    "    \n",
    "    layers.append(Linear(n_neurons_per_layer, 10))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your objective with optuna hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best hyperparameters for\n",
    "# 1) the amount of hidden layers\n",
    "# 2) the neurons per hidden layer\n",
    "# 3) batch size\n",
    "# 4) learning rate\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = build_model(trial).to(DEVICE)\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader, test_loader = load_mnist_data(batch_size=batch_size)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(EPOCHS), desc='Iterating epoch'):\n",
    "        t_avg_loss, t_acc = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            dataloader=train_loader\n",
    "        )\n",
    "\n",
    "        e_avg_loss, e_acc = eval_model(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            dataloader=train_loader\n",
    "        )\n",
    "\n",
    "        trial.report(e_acc, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return e_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-27 04:15:02,900] A new study created in RDB with name: example-study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 9912422/9912422 [00:01<00:00, 9046688.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 33070077.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 7232735.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 902654.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [15:49<00:00, 94.98s/it]\n",
      "[I 2023-12-27 04:30:56,102] Trial 0 finished with value: 0.8635833263397217 and parameters: {'n_layers': 1, 'n_neurons': 55, 'lr': 0.020480115443510482, 'batch_size': 8}. Best is trial 0 with value: 0.8635833263397217.\n",
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [15:45<00:00, 94.50s/it]\n",
      "[I 2023-12-27 04:46:41,405] Trial 1 finished with value: 0.9192500114440918 and parameters: {'n_layers': 1, 'n_neurons': 17, 'lr': 4.179381868033112e-05, 'batch_size': 4}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [10:41<00:00, 64.19s/it]\n",
      "[I 2023-12-27 04:57:23,672] Trial 2 finished with value: 0.8551666736602783 and parameters: {'n_layers': 5, 'n_neurons': 54, 'lr': 0.008077144706907966, 'batch_size': 8}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [07:49<00:00, 47.00s/it]\n",
      "[I 2023-12-27 05:05:13,970] Trial 3 finished with value: 0.9186999797821045 and parameters: {'n_layers': 3, 'n_neurons': 60, 'lr': 0.00020166335943276357, 'batch_size': 16}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [16:05<00:00, 96.51s/it]\n",
      "[I 2023-12-27 05:21:19,355] Trial 4 finished with value: 0.9037833213806152 and parameters: {'n_layers': 5, 'n_neurons': 20, 'lr': 0.0011402148474079737, 'batch_size': 4}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch:   0%|                                                            | 0/10 [01:02<?, ?it/s]\n",
      "[I 2023-12-27 05:22:21,814] Trial 5 pruned. \n",
      "Iterating epoch: 100%|██████████████████████████████████████████████████| 10/10 [17:10<00:00, 103.01s/it]\n",
      "[I 2023-12-27 05:39:32,166] Trial 6 finished with value: 0.9088000059127808 and parameters: {'n_layers': 5, 'n_neurons': 28, 'lr': 0.000397054603983472, 'batch_size': 4}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch: 100%|██████████████████████████████████████████████████| 10/10 [52:30<00:00, 315.07s/it]\n",
      "[I 2023-12-27 06:32:03,198] Trial 7 finished with value: 0.9164000153541565 and parameters: {'n_layers': 1, 'n_neurons': 63, 'lr': 0.0008454956147422707, 'batch_size': 4}. Best is trial 1 with value: 0.9192500114440918.\n",
      "Iterating epoch:   0%|                                                            | 0/10 [00:52<?, ?it/s]\n",
      "[I 2023-12-27 06:32:56,207] Trial 8 pruned. \n",
      "Iterating epoch: 100%|███████████████████████████████████████████████████| 10/10 [07:43<00:00, 46.38s/it]\n",
      "[I 2023-12-27 06:40:40,334] Trial 9 finished with value: 0.9210833311080933 and parameters: {'n_layers': 5, 'n_neurons': 39, 'lr': 0.00024752637772860813, 'batch_size': 16}. Best is trial 9 with value: 0.9210833311080933.\n",
      "Iterating epoch:   0%|                                                            | 0/10 [00:47<?, ?it/s]\n",
      "[I 2023-12-27 06:41:28,398] Trial 10 pruned. \n",
      "Iterating epoch:   0%|                                                            | 0/10 [41:02<?, ?it/s]\n",
      "[I 2023-12-27 07:22:31,410] Trial 11 pruned. \n",
      "Iterating epoch: 100%|██████████████████████████████████████████████████| 10/10 [22:09<00:00, 132.99s/it]\n",
      "[I 2023-12-27 07:44:41,854] Trial 12 finished with value: 0.9232000112533569 and parameters: {'n_layers': 2, 'n_neurons': 39, 'lr': 8.330760120488075e-05, 'batch_size': 16}. Best is trial 12 with value: 0.9232000112533569.\n",
      "Iterating epoch: 100%|██████████████████████████████████████████████████| 10/10 [58:34<00:00, 351.42s/it]\n",
      "[I 2023-12-27 08:43:16,783] Trial 13 finished with value: 0.920116662979126 and parameters: {'n_layers': 2, 'n_neurons': 38, 'lr': 0.00010248371960897031, 'batch_size': 16}. Best is trial 12 with value: 0.9232000112533569.\n",
      "Iterating epoch:   0%|                                                            | 0/10 [02:26<?, ?it/s]\n",
      "[I 2023-12-27 08:45:44,482] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  15\n",
      "  Number of pruned trials:  5\n",
      "  Number of complete trials:  10\n",
      "Best trial:\n",
      "  Value:  0.9232000112533569\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_neurons: 39\n",
      "    lr: 8.330760120488075e-05\n",
      "    batch_size: 16\n"
     ]
    }
   ],
   "source": [
    "from optuna.trial import TrialState\n",
    "\n",
    "# Add stream handler of stdout to show the messages\n",
    "study_name = \"example-study\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize')\n",
    "\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the optuna trial with optuna-dashboard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You can open the optuna dashboard by typing \n",
    "\"optuna-dashboard sqlite:///path_to_your_file.db\"\n",
    "into your command line (python environment)\n",
    "\n",
    "\n",
    "Which model is best?\n",
    "\n",
    "Check the dashboard for more details"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
