{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI   \n",
      "0              6      148             72             35        0  33.6  \\\n",
      "1              1       85             66             29        0  26.6   \n",
      "2              8      183             64              0        0  23.3   \n",
      "3              1       89             66             23       94  28.1   \n",
      "4              0      137             40             35      168  43.1   \n",
      "..           ...      ...            ...            ...      ...   ...   \n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                       0.627   50        1  \n",
      "1                       0.351   31        0  \n",
      "2                       0.672   32        1  \n",
      "3                       0.167   21        0  \n",
      "4                       2.288   33        1  \n",
      "..                        ...  ...      ...  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "\n",
      "[768 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X = data.iloc[:, :-1]  \n",
    "y = data.iloc[:, -1] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                   1.000\n",
      "Glucose                     109.000\n",
      "BloodPressure                38.000\n",
      "SkinThickness                18.000\n",
      "Insulin                     120.000\n",
      "BMI                          23.100\n",
      "DiabetesPedigreeFunction      0.407\n",
      "Age                          26.000\n",
      "Name: 599, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_index = random.randint(0, len(X_train) - 1)\n",
    "random_sample = X_train.iloc[random_index]\n",
    "print(random_sample)\n",
    "print(y_train.iloc[93])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "W1 = torch.nn.init.xavier_normal_(torch.empty(8,10))\n",
    "W2 = torch.nn.init.xavier_normal_(torch.empty(10,1))\n",
    "b1 = np.random.rand(10)\n",
    "b2 = np.random.rand(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code   \n",
    "import math\n",
    "def Loss_derivative(target_output , output):\n",
    "    return (output - target_output) / (output * (1-output))\n",
    "\n",
    "def Loss(target_output , output):\n",
    "    return ((-target_output * math.log(output)) - ((1-target_output) * math.log(1-output)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return torch.sigmoid(x) * (1 - torch.sigmoid(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      " Loss: 0.02176114072370852\n",
      "Iteration 1: \n",
      " Loss: 0.009330327244129705\n",
      "Iteration 2: \n",
      " Loss: 3.6203820385110257\n",
      "Iteration 3: \n",
      " Loss: 0.034635588810400325\n",
      "Iteration 4: \n",
      " Loss: 4.1264806845784605\n",
      "Iteration 5: \n",
      " Loss: 3.549333282123845\n",
      "Iteration 6: \n",
      " Loss: 0.07429073642270519\n",
      "Iteration 7: \n",
      " Loss: 2.8813714966609165\n",
      "Iteration 8: \n",
      " Loss: 0.13317104986831166\n",
      "Iteration 9: \n",
      " Loss: 0.12560669045548384\n",
      "Iteration 10: \n",
      " Loss: 2.6926602212481976\n",
      "Iteration 11: \n",
      " Loss: 1.5682336060147744\n",
      "Iteration 12: \n",
      " Loss: 0.3044326522952092\n",
      "Iteration 13: \n",
      " Loss: 1.9604898473619263\n",
      "Iteration 14: \n",
      " Loss: 1.535686128830551\n",
      "Iteration 15: \n",
      " Loss: 0.3642133303709646\n",
      "Iteration 16: \n",
      " Loss: 1.3173302947205894\n",
      "Iteration 17: \n",
      " Loss: 0.9861192369428619\n",
      "Iteration 18: \n",
      " Loss: 0.7685361565719053\n",
      "Iteration 19: \n",
      " Loss: 0.8147035494169316\n",
      "Iteration 20: \n",
      " Loss: 0.7764168069170708\n",
      "Iteration 21: \n",
      " Loss: 0.5901523204055866\n",
      "Iteration 22: \n",
      " Loss: 0.4629086189770452\n",
      "Iteration 23: \n",
      " Loss: 0.3744478250836414\n",
      "Iteration 24: \n",
      " Loss: 0.3131969376316137\n",
      "Iteration 25: \n",
      " Loss: 0.6527816298615631\n",
      "Iteration 26: \n",
      " Loss: 0.3527271708509569\n",
      "Iteration 27: \n",
      " Loss: 0.2952751366340199\n",
      "Iteration 28: \n",
      " Loss: 0.2524749496117843\n",
      "Iteration 29: \n",
      " Loss: 0.21967903682971354\n",
      "Iteration 30: \n",
      " Loss: 0.1939179661371611\n",
      "Iteration 31: \n",
      " Loss: 1.8384301237335468\n",
      "Iteration 32: \n",
      " Loss: 0.2932475717770722\n",
      "Iteration 33: \n",
      " Loss: 0.25094032590461907\n",
      "Iteration 34: \n",
      " Loss: 0.21848654030241782\n",
      "Iteration 35: \n",
      " Loss: 0.1929698421997257\n",
      "Iteration 36: \n",
      " Loss: 1.8425048996545377\n",
      "Iteration 37: \n",
      " Loss: 1.3730580470922897\n",
      "Iteration 38: \n",
      " Loss: 0.45247966007587825\n",
      "Iteration 39: \n",
      " Loss: 1.180103685779731\n",
      "Iteration 40: \n",
      " Loss: 0.5424955860185505\n",
      "Iteration 41: \n",
      " Loss: 1.0512192818271857\n",
      "Iteration 42: \n",
      " Loss: 0.613658447161349\n",
      "Iteration 43: \n",
      " Loss: 0.9659362219345444\n",
      "Iteration 44: \n",
      " Loss: 0.6668387153857468\n",
      "Iteration 45: \n",
      " Loss: 0.5154283976011507\n",
      "Iteration 46: \n",
      " Loss: 0.41123838937592744\n",
      "Iteration 47: \n",
      " Loss: 0.3377154744205947\n",
      "Iteration 48: \n",
      " Loss: 0.2842114823336593\n",
      "Iteration 49: \n",
      " Loss: 1.5298212921018504\n",
      "Iteration 50: \n",
      " Loss: 0.39094872922748286\n",
      "Iteration 51: \n",
      " Loss: 0.32311204480183325\n",
      "Iteration 52: \n",
      " Loss: 0.27337076322448595\n",
      "Iteration 53: \n",
      " Loss: 0.23579877740641414\n",
      "Iteration 54: \n",
      " Loss: 0.20665397810605035\n",
      "Iteration 55: \n",
      " Loss: 0.18351592535867411\n",
      "Iteration 56: \n",
      " Loss: 0.16477498074201974\n",
      "Iteration 57: \n",
      " Loss: 0.1493308152425506\n",
      "Iteration 58: \n",
      " Loss: 0.1364115445858655\n",
      "Iteration 59: \n",
      " Loss: 2.13782218537019\n",
      "Iteration 60: \n",
      " Loss: 0.22142884736765966\n",
      "Iteration 61: \n",
      " Loss: 1.7292440424070397\n",
      "Iteration 62: \n",
      " Loss: 0.32469590856118313\n",
      "Iteration 63: \n",
      " Loss: 0.5960888318739448\n",
      "Iteration 64: \n",
      " Loss: 0.3595932201281875\n",
      "Iteration 65: \n",
      " Loss: 0.30172197104501586\n",
      "Iteration 66: \n",
      " Loss: 1.487677274485739\n",
      "Iteration 67: \n",
      " Loss: 0.7331233277778286\n",
      "Iteration 68: \n",
      " Loss: 0.8693961002110152\n",
      "Iteration 69: \n",
      " Loss: 0.7342160673353737\n",
      "Iteration 70: \n",
      " Loss: 0.5211978846368777\n",
      "Iteration 71: \n",
      " Loss: 0.6935314221956314\n",
      "Iteration 72: \n",
      " Loss: 0.8860549052202906\n",
      "Iteration 73: \n",
      " Loss: 0.38342816520754613\n",
      "Iteration 74: \n",
      " Loss: 0.3176699623967342\n",
      "Iteration 75: \n",
      " Loss: 0.2693102259924359\n",
      "Iteration 76: \n",
      " Loss: 0.40505672061016473\n",
      "Iteration 77: \n",
      " Loss: 1.7611763890186964\n",
      "Iteration 78: \n",
      " Loss: 0.3151723026202382\n",
      "Iteration 79: \n",
      " Loss: 0.4480975571441819\n",
      "Iteration 80: \n",
      " Loss: 1.212341815805214\n",
      "Iteration 81: \n",
      " Loss: 0.3288546304628039\n",
      "Iteration 82: \n",
      " Loss: 0.9699792957256966\n",
      "Iteration 83: \n",
      " Loss: 1.1063627311796207\n",
      "Iteration 84: \n",
      " Loss: 0.5877536559096164\n",
      "Iteration 85: \n",
      " Loss: 1.1792059221115665\n",
      "Iteration 86: \n",
      " Loss: 0.8303003896093114\n",
      "Iteration 87: \n",
      " Loss: 0.5156932161990472\n",
      "Iteration 88: \n",
      " Loss: 0.9589345481140369\n",
      "Iteration 89: \n",
      " Loss: 0.684287266046197\n",
      "Iteration 90: \n",
      " Loss: 0.9215644232052321\n",
      "Iteration 91: \n",
      " Loss: 0.7274376250467633\n",
      "Iteration 92: \n",
      " Loss: 0.8810609008588658\n",
      "Iteration 93: \n",
      " Loss: 0.7562642433902955\n",
      "Iteration 94: \n",
      " Loss: 0.8554935784681552\n",
      "Iteration 95: \n",
      " Loss: 0.6172826165270108\n",
      "Iteration 96: \n",
      " Loss: 0.9906129718285285\n",
      "Iteration 97: \n",
      " Loss: 0.6814597121338526\n",
      "Iteration 98: \n",
      " Loss: 0.9243194628937774\n",
      "Iteration 99: \n",
      " Loss: 0.7255291585023164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8532\\3869064458.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z1 = torch.tensor(z1)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8532\\3869064458.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b2 = torch.tensor(b2)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8532\\3869064458.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b1 = torch.tensor(b1) - learning_rate * dlossdb1\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached\n",
    "\n",
    "\n",
    "epoch = 100\n",
    "learning_rate = 0.1\n",
    "for i in range(epoch):\n",
    "\n",
    "    #forward propagation\n",
    "    x = random.randint(0,767)\n",
    "    random_index = random.randint(0, len(X_train) - 1)\n",
    "    input = X_train.iloc[random_index] #(8,)\n",
    "    target_output = y_train.iloc[random_index] #(1)\n",
    "\n",
    "    #hidden layer\n",
    "    input = input.values\n",
    "    z1 = (W1.T @ input) + b1  #(10,)\n",
    "    z1 = torch.tensor(z1)\n",
    "    a1 = torch.sigmoid(z1) #(10,)\n",
    "    \n",
    "\n",
    "    #output layer\n",
    "    a1 = a1.view(-1).double()\n",
    "    W2 = W2.view(-1).double() #(10,)\n",
    "    z2 = torch.dot(W2, a1) + b2 #(1)\n",
    "    output = torch.sigmoid(z2) #(1)\n",
    "\n",
    "    #loss\n",
    "    loss = Loss(target_output, output)\n",
    "\n",
    "\n",
    "    #backpropagation\n",
    "    #output layer\n",
    "    dlossdoutput = Loss_derivative(target_output, output) #(1)\n",
    "    doutputdz2 = sigmoid_derivative(z2)  #(1)\n",
    "    dz2dw2 = a1 #(10)\n",
    "    dz2db2 = 1\n",
    "    dlossdw2 = dlossdoutput * doutputdz2 * dz2dw2 #(10)\n",
    "    dlossdb2 = dlossdoutput * doutputdz2 * dz2db2 #(1)\n",
    "     \n",
    "    W2 = W2 - learning_rate * dlossdw2 #(10)\n",
    "    b2 = torch.tensor(b2)\n",
    "    b2 = b2 - learning_rate * dlossdb2 #(1)\n",
    "\n",
    "    #hidden layer\n",
    "    dlossdz2 = dlossdoutput * doutputdz2 #(1)\n",
    "    dz2da1 = W2 #(10)\n",
    "    da1dz1 = sigmoid_derivative(z1) #(10)\n",
    "    dz1dw1 = input\n",
    "    dz1db1 = 1\n",
    "    dlossdz1 = dlossdz2 * dz2da1 * da1dz1\n",
    "    dlossdz1 = dlossdz1.numpy()\n",
    "    dlossdw1 = np.outer(dz1dw1, dlossdz1)\n",
    "    dlossdb1 = dlossdz2 * dz2da1 * da1dz1 * dz1db1\n",
    "    \n",
    "    W1 = W1 - learning_rate * dlossdw1\n",
    "    b1 = torch.tensor(b1) - learning_rate * dlossdb1\n",
    "\n",
    "\n",
    "    print(f\"Iteration {i}: \\n Loss: {loss}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('aai2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9621b288640ec72265c2f0c931c1129106da1ad09ceb1028ae8cec1cf5ff8f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
