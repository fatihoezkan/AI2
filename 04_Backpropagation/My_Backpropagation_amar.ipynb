{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyImporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Prepare your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13476\\2597183206.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./diabetes.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# target = pd.DataFrame(df.target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ammar\\anaconda3\\envs\\AI2\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "mnist = load_digits()\n",
    "head = pd.DataFrame(mnist.data).head()\n",
    "target = pd.DataFrame(mnist.target)\n",
    "\n",
    "X = mnist.data\n",
    "y = target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "y_train = torch.nn.functional.one_hot(y_train)\n",
    "y_test = torch.nn.functional.one_hot(y_test)\n",
    "\n",
    "# CONTENT AND DIMENTION CHECK\n",
    "\n",
    "# print(y_train.shape)\n",
    "\n",
    "# mnist.keys() -> dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n",
    "\n",
    "# print(f'Pure Instance 1018 of the dataset: {X_train[1018]} \\n with \\n {y_train.iloc[1018]} \\n and the 427 insatnce: {X_train[427]} \\n with \\n {y_train.iloc[427]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix W1 shape: torch.Size([8, 64]) \n",
      " Matrix W2 shape: torch.Size([10, 8]) \n",
      " Bias1 shape: torch.Size([8]) \n",
      " Bias2 shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "W1 = nn.init.xavier_normal_(torch.empty(8,64))\n",
    "W2 = nn.init.xavier_normal_(torch.empty(10,8))\n",
    "\n",
    "b1 = np.random.randn(8)\n",
    "b2 = np.random.randn(10)\n",
    "\n",
    "b1 = torch.tensor(b1,dtype=torch.float32)\n",
    "b2 = torch.tensor(b2, dtype=torch.float32)\n",
    "\n",
    "# CONTENT AND DIMENTION CHECK\n",
    "\n",
    "# print(f'Matrix W1: {W1} \\n Matrix W2: {W2} \\n Bias1: {b1} \\n Bias2: {b2}')\n",
    "print(f'Matrix W1 shape: {W1.shape} \\n Matrix W2 shape: {W2.shape} \\n Bias1 shape: {b1.shape} \\n Bias2 shape: {b2.shape}')\n",
    "\n",
    "# print(f'{W1.shape} X {X.shape}')\n",
    "# print(f'Result is {W1.T * X}')\n",
    "\n",
    "# print(f'Layer 1: \\n\\t Weights: \\n \\tNormal [first ten rows only]: \\n\\n {W1[:10,:]} \\n\\n \\t Transposed: \\n\\n {W1.T}')\n",
    "# print(f'Layer 1: \\n\\t Weights: \\n \\tNormal [first ten rows only]: \\n\\n {W2[:10,:]} \\n\\n \\t Transposed: \\n\\n {W2.T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "# cel = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def MyCrossEntropyLoss(y,y_hat):\n",
    "    # lst = []\n",
    "    # for i in range(len(y)):\n",
    "    #     element = y[i] * np.log(yhat[i])\n",
    "    #     lst.append(element)\n",
    "    # return -np.sum(lst)\n",
    "\n",
    "    return torch.sum(-y * torch.log(y_hat))\n",
    "\n",
    "def sigmoid_deriv(grad_output, sigmoid_output):\n",
    "    return grad_output * sigmoid_output * (1 - sigmoid_output)\n",
    "\n",
    "def softmax_deriv(grad_output, softmax_output):\n",
    "    return grad_output * softmax_output * (1 - softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DERIVATIVES 1: \n",
      " dLdout:tensor([[ 0.2007, -0.8818,  0.1681,  0.0089,  0.0467,  0.0616,  0.2160,  0.1172,\n",
      "          0.0211,  0.0416]]) with shape torch.Size([1, 10]) -> \n",
      " doutdZ2:0.8347920775413513 with shape torch.Size([]) -> \n",
      " dZ2dW2:tensor([9.8556e-01, 5.2566e-01, 3.6379e-01, 1.6674e-03, 1.0929e-02, 1.4696e-11,\n",
      "        1.0000e+00, 1.3319e-02]) with shape torch.Size([8]) -> \n",
      " dZ2db2:1 with no shape as its an int \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammar\\AppData\\Local\\Temp\\ipykernel_15744\\688790443.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  OUTPUT = nn.functional.softmax(Z2)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached\n",
    "\n",
    "# IMPLEMENTATION\n",
    "\n",
    "learning_rate = 0.05\n",
    "learning_rate = torch.tensor(learning_rate, dtype=torch.float32)\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    # VISUALIZATION\n",
    "    # print(f'BEGGIN: \\n X: {x_rand, x_rand.shape} \\n W1: {W1,W1.shape} \\n b1: {b1, b1.shape} \\n W2: {W2, W2.shape} \\n b2: {b2, b2.shape}')\n",
    "\n",
    "    # FORWARD\n",
    "    random.seed(25)\n",
    "    rnd = random.randint(0,len(X_train)) # -> 772th image\n",
    "    # print(f'Random indice {rnd}')\n",
    "\n",
    "    x_rand = X_train[rnd]\n",
    "    x_rand = torch.tensor(x_rand, dtype=torch.float32) # Shape (1,64)\n",
    "\n",
    "    target_rand = y_train[rnd] # -> target = 5 for random.seed(25)\n",
    "    # print(target_rand)\n",
    "\n",
    "    # MODEL\n",
    "    Z1 = torch.matmul(x_rand, W1.T) + b1\n",
    "    A1 = torch.sigmoid(Z1)\n",
    "    Z2 = torch.matmul(A1, W2.T) + b2\n",
    "    OUTPUT = nn.functional.softmax(Z2)\n",
    "\n",
    "    # LOSS\n",
    "    L = MyCrossEntropyLoss(target_rand,OUTPUT)\n",
    "    # print(f'Z1: {Z1.shape} \\n A1: {A1.shape} \\n Z2: {Z2.shape} \\n Output: {OUTPUT.shape} \\n Loss: {L}')\n",
    "    # # print(L)\n",
    "\n",
    "    # # BACKPROPAGATION (Layer 2-1)\n",
    "    dLdout = (OUTPUT - target_rand) / torch.matmul(OUTPUT,(1 - OUTPUT)) # entropy derivative 1x10\n",
    "    doutdZ2 = OUTPUT * (1-OUTPUT) # Softmax derivative\n",
    "    dZ2dW2 = A1.T # shape 8,\n",
    "    dZ2db2 = 1 # no shape as its an int\n",
    "    # print(f'DERIVATIVES 1: \\n dLdout:{dLdout} with shape {dLdout.shape} -> \\n doutdZ2:{doutdZ2} with shape {doutdZ2.shape} -> \\n dZ2dW2:{dZ2dW2} with shape {dZ2dW2.shape} -> \\n dZ2db2:{dZ2db2} with no shape as its an int \\n')\n",
    "\n",
    "    # GRADIENTS\n",
    "    result_of_grads = dLdout * doutdZ2\n",
    "    gradb2 = dZ2db2 * result_of_grads # Shape 1,\n",
    "    gradW2 = dZ2dW2 * result_of_grads.T # Shape 8,\n",
    "    # # print(f'GRADIENTS 1: \\n Gradb2: {gradb2} and shape {gradb2.shape} \\n GradW2: {gradW2} and shape {gradW2.shape}')\n",
    "\n",
    "    # # BACKPROPAGATION (Layer 1-0)\n",
    "    dLdz2 = dLdout * doutdZ2 # 1x10\n",
    "    dZ2dA1 = W2 # 10x8\n",
    "    dA1dZ1 = A1 * (1-A1) # 1x8\n",
    "    dZ1dW1 = x_rand # 1x64\n",
    "    dZ1db1 = 1\n",
    "    # # print(f'DERIVATIVES 2: \\n dLdZ2:{dLdz2} with shape {dLdz2.shape} -> \\n dZ2dA1:{dZ2dA1} with shape {dZ2dA1.shape} -> \\n dA1dZ1:{dA1dZ1} with shape {dA1dZ1.shape} -> \\n dZ1dW1:{dZ1dW1} with shape {dZ1dW1.shape} -> \\n dZ1db1:{dZ1db1} with no shape as its an int\\n')\n",
    "\n",
    "    # # GRADIENTS\n",
    "    gradb1 = ((dZ1db1 * dA1dZ1) @ dZ2dA1.T) * dLdz2 # 1x10\n",
    "    gradW1 = ((dZ1dW1 * dA1dZ1) @ dZ2dA1.T) * dLdz2 \n",
    "    # # print(f'GRADIENTS 2: \\n gradb1: {gradb1} and shape {gradb1.shape} \\n gradW1: {gradW1} and shape {gradW1.shape}')\n",
    "\n",
    "    # # UPDATES\n",
    "    b2 = b2 - (learning_rate * gradb2) # 1x10 - (1x10) = 1x10\n",
    "    W2 = W2 - (learning_rate * gradW2)\n",
    "    # print(f'UPDATES 1: \\n b2: {b2} with shape {b2.shape} \\n W2: {W2} with shape {W2.shape}')\n",
    "    \n",
    "    # # UPDATES\n",
    "    b1 = b1 - (learning_rate * gradb1) # Shape\n",
    "    W1 = W1 - (learning_rate * gradW1) # Shape\n",
    "    # print(f'UPDATES 2: \\n b1: {b1} with shape {b1.shape} \\n W1: {W1} with shape {W1.shape}')\n",
    "\n",
    "    # print(f'END: \\n X: {x_rand, x_rand.shape} \\n W1: {W1,W1.shape} \\n b1: {b1, b1.shape} \\n W2: {W2, W2.shape} \\n b2: {b2, b2.shape}')\n",
    "[random_point,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('aai2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9621b288640ec72265c2f0c931c1129106da1ad09ceb1028ae8cec1cf5ff8f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
