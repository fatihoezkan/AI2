{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer \n",
    "(you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import numpy as np\n",
    "\n",
    "# def load_mnist_data_for_network(root_path='./data', batch_size=4):\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5,), (0.5,))\n",
    "#     ])\n",
    "\n",
    "#     trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "#     trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#     testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "#     testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "#     # Convert PyTorch DataLoader to NumPy arrays\n",
    "#     train_images, train_labels = [], []\n",
    "#     for data in trainloader:\n",
    "#         inputs, labels = data\n",
    "#         # Reshape each image to (728,)\n",
    "#         train_images.append(inputs.numpy().reshape(-1, 28*28))\n",
    "#         train_labels.append(labels.numpy())\n",
    "\n",
    "#     test_images, test_labels = [], []\n",
    "#     for data in testloader:\n",
    "#         inputs, labels = data\n",
    "#         # Reshape each image to (728,)\n",
    "#         test_images.append(inputs.numpy().reshape(-1, 28*28))\n",
    "#         test_labels.append(labels.numpy())\n",
    "\n",
    "#     # Concatenate batches into NumPy arrays\n",
    "#     train_images = np.concatenate(train_images, axis=0)\n",
    "#     train_labels = np.concatenate(train_labels, axis=0)\n",
    "#     test_images = np.concatenate(test_images, axis=0)\n",
    "#     test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "#     return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "# train_data, test_data = load_mnist_data_for_network()\n",
    "# print(train_data[0].shape)  # Shape of the train images (num_samples, 728)\n",
    "# print(train_data[1].shape)  # Shape of the train labels (num_samples,)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TrainData = pd.read_csv(\"./data/mnist_train.csv\")\n",
    "TestData = pd.read_csv(\"./data/mnist_test.csv\")\n",
    "\n",
    "X_train = TrainData.iloc[:,:-1]\n",
    "X_test = TestData.iloc[:,:-1]\n",
    "\n",
    "y_train = TrainData.iloc[:,-1]\n",
    "y_test = TestData.iloc[:,-1]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 32) (32, 10) (32,) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "import numpy as np\n",
    "def xavier_init(shape = tuple):\n",
    "    if len(shape) == 1:\n",
    "        number_of_in, number_of_out = shape[0], shape[0]\n",
    "    elif len(shape) == 2:\n",
    "        number_of_in, number_of_out = shape[0], shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Xavier initialization supports only 1D and 2D shapes.\")\n",
    "\n",
    "    scaler= np.sqrt(2.0 / (number_of_in + number_of_out))\n",
    "\n",
    "    return np.random.normal(loc=0.0, scale=scaler, size=shape)\n",
    "\n",
    "def bias(shape = tuple):\n",
    "    bias = np.random.normal(loc=0.0, size = shape[1] )\n",
    "    return bias\n",
    "\n",
    "\n",
    "W1 = xavier_init((784,32))\n",
    "W2 = xavier_init((32,10))\n",
    "B1 = bias((784,32))\n",
    "B2 = bias((32,10))\n",
    "\n",
    "print(W1.shape, W2.shape, B1.shape , B2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "  \n",
    "#LOSS FUNC \\ cross_entropy\n",
    "def Loss_derivative(target_output , output):\n",
    "    return (output - target_output) / (output * (1-output))\n",
    "\n",
    "def Loss(target_output , output): #cross_entropy\n",
    "    return ((-target_output * np.log(output)) - ((1-target_output) * np.log(1-output)))\n",
    "\n",
    "#activation function\n",
    "def sigmoid(input): \n",
    "     return (1 / (1+np.exp(-1 * input)))\n",
    "\n",
    "#activision funct derivative\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      " Loss: 0.6690064090259646\n",
      "Iteration 1: \n",
      " Loss: 0.6789447483949365\n",
      "Iteration 2: \n",
      " Loss: 0.6454990767215273\n",
      "Iteration 3: \n",
      " Loss: 0.6879373794835713\n",
      "Iteration 4: \n",
      " Loss: 0.5648131957944088\n",
      "Iteration 5: \n",
      " Loss: 0.6285465501042136\n",
      "Iteration 6: \n",
      " Loss: 0.5400467076852211\n",
      "Iteration 7: \n",
      " Loss: 0.5010356794739252\n",
      "Iteration 8: \n",
      " Loss: 0.4129859332801156\n",
      "Iteration 9: \n",
      " Loss: 0.4474753718023613\n",
      "Iteration 10: \n",
      " Loss: 0.42367393610716847\n",
      "Iteration 11: \n",
      " Loss: 0.5154237183226378\n",
      "Iteration 12: \n",
      " Loss: 0.3538823097671428\n",
      "Iteration 13: \n",
      " Loss: 0.41945949416628536\n",
      "Iteration 14: \n",
      " Loss: 0.3422127671683622\n",
      "Iteration 15: \n",
      " Loss: 0.3446476364165724\n",
      "Iteration 16: \n",
      " Loss: 0.29975076130816986\n",
      "Iteration 17: \n",
      " Loss: 0.27177668440704783\n",
      "Iteration 18: \n",
      " Loss: 0.32949663317586453\n",
      "Iteration 19: \n",
      " Loss: 0.28020705982051\n",
      "Iteration 20: \n",
      " Loss: 0.31141068876629463\n",
      "Iteration 21: \n",
      " Loss: 0.23281571217784586\n",
      "Iteration 22: \n",
      " Loss: 0.2615301208045351\n",
      "Iteration 23: \n",
      " Loss: 0.21664123694496668\n",
      "Iteration 24: \n",
      " Loss: 0.20224338156193583\n",
      "Iteration 25: \n",
      " Loss: 0.20435758644414354\n",
      "Iteration 26: \n",
      " Loss: 0.2253696028127396\n",
      "Iteration 27: \n",
      " Loss: 0.23109328298823\n",
      "Iteration 28: \n",
      " Loss: 0.16705066353619477\n",
      "Iteration 29: \n",
      " Loss: 0.1857626855573346\n",
      "Iteration 30: \n",
      " Loss: 0.2243809159759405\n",
      "Iteration 31: \n",
      " Loss: 0.18134055654268685\n",
      "Iteration 32: \n",
      " Loss: 0.188624565150562\n",
      "Iteration 33: \n",
      " Loss: 0.17516646601996697\n",
      "Iteration 34: \n",
      " Loss: 0.21889442609283397\n",
      "Iteration 35: \n",
      " Loss: 0.15896454441785957\n",
      "Iteration 36: \n",
      " Loss: 0.1625334667533101\n",
      "Iteration 37: \n",
      " Loss: 0.1494638772596089\n",
      "Iteration 38: \n",
      " Loss: 0.17026264886215353\n",
      "Iteration 39: \n",
      " Loss: 0.1437467371183089\n",
      "Iteration 40: \n",
      " Loss: 0.13370961458943872\n",
      "Iteration 41: \n",
      " Loss: 0.12890747085966586\n",
      "Iteration 42: \n",
      " Loss: 0.17451985589243227\n",
      "Iteration 43: \n",
      " Loss: 0.11303553892925462\n",
      "Iteration 44: \n",
      " Loss: 0.14451916752178562\n",
      "Iteration 45: \n",
      " Loss: 0.10807781813331244\n",
      "Iteration 46: \n",
      " Loss: 0.14146817931487016\n",
      "Iteration 47: \n",
      " Loss: 0.12591405496625976\n",
      "Iteration 48: \n",
      " Loss: 0.12119687082139582\n",
      "Iteration 49: \n",
      " Loss: 0.09883418998989842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ynzbkd_s00db7zl4yrnnry7h0000gn/T/ipykernel_2634/551249204.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return (1 / (1+np.exp(-1 * input)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: \n",
      " Loss: 0.10343280842836686\n",
      "Iteration 51: \n",
      " Loss: 0.16312653301209795\n",
      "Iteration 52: \n",
      " Loss: 0.12039698078057168\n",
      "Iteration 53: \n",
      " Loss: 0.12083603217567496\n",
      "Iteration 54: \n",
      " Loss: 0.0922438003242287\n",
      "Iteration 55: \n",
      " Loss: 0.09167341526817245\n",
      "Iteration 56: \n",
      " Loss: 0.08624220083999423\n",
      "Iteration 57: \n",
      " Loss: 0.08486003683183231\n",
      "Iteration 58: \n",
      " Loss: 0.08633015475827727\n",
      "Iteration 59: \n",
      " Loss: 0.0985278185608047\n",
      "Iteration 60: \n",
      " Loss: 0.08067786475728375\n",
      "Iteration 61: \n",
      " Loss: 0.0794345664069609\n",
      "Iteration 62: \n",
      " Loss: 0.07822100873382701\n",
      "Iteration 63: \n",
      " Loss: 0.0770523290994118\n",
      "Iteration 64: \n",
      " Loss: 0.07854259604019202\n",
      "Iteration 65: \n",
      " Loss: 0.07483552545741701\n",
      "Iteration 66: \n",
      " Loss: 0.09032807509247515\n",
      "Iteration 67: \n",
      " Loss: 0.08714320110444596\n",
      "Iteration 68: \n",
      " Loss: 0.07141001363386584\n",
      "Iteration 69: \n",
      " Loss: 0.08620376195793641\n",
      "Iteration 70: \n",
      " Loss: 0.08480208641419354\n",
      "Iteration 71: \n",
      " Loss: 0.09063917691503148\n",
      "Iteration 72: \n",
      " Loss: 0.07242259983764861\n",
      "Iteration 73: \n",
      " Loss: 0.06625805605260648\n",
      "Iteration 74: \n",
      " Loss: 0.06771538042317873\n",
      "Iteration 75: \n",
      " Loss: 0.07749496209690997\n",
      "Iteration 76: \n",
      " Loss: 0.06370288196200011\n",
      "Iteration 77: \n",
      " Loss: 0.0629276831398549\n",
      "Iteration 78: \n",
      " Loss: 0.06217098263079449\n",
      "Iteration 79: \n",
      " Loss: 0.06143212642635039\n",
      "Iteration 80: \n",
      " Loss: 0.06272388861314748\n",
      "Iteration 81: \n",
      " Loss: 0.058632278046809795\n",
      "Iteration 82: \n",
      " Loss: 0.06406394519211103\n",
      "Iteration 83: \n",
      " Loss: 0.06326946214144999\n",
      "Iteration 84: \n",
      " Loss: 0.05645020898175751\n",
      "Iteration 85: \n",
      " Loss: 0.07602712209168709\n",
      "Iteration 86: \n",
      " Loss: 0.061774711394952365\n",
      "Iteration 87: \n",
      " Loss: 0.05767419179912214\n",
      "Iteration 88: \n",
      " Loss: 0.05987119741353051\n",
      "Iteration 89: \n",
      " Loss: 0.06327469304245699\n",
      "Iteration 90: \n",
      " Loss: 0.05420606458304543\n",
      "Iteration 91: \n",
      " Loss: 0.05967987885506891\n",
      "Iteration 92: \n",
      " Loss: 0.054810458771187585\n",
      "Iteration 93: \n",
      " Loss: 0.05299961454583017\n",
      "Iteration 94: \n",
      " Loss: 0.05609437463852131\n",
      "Iteration 95: \n",
      " Loss: 0.0512474378276815\n",
      "Iteration 96: \n",
      " Loss: 0.053896096054271744\n",
      "Iteration 97: \n",
      " Loss: 0.050479272623245405\n",
      "Iteration 98: \n",
      " Loss: 0.050356714243353994\n",
      "Iteration 99: \n",
      " Loss: 0.049507255635377764\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached\n",
    "\n",
    "import random\n",
    "\n",
    "# input_data = train_data[0][1]\n",
    "# targets = train_data[1][1]\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "#forward pass\n",
    "\n",
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "\n",
    "    random_point = random.randint(0,len(X_train))\n",
    "    \n",
    "    input_data = X_train.loc[random_point,:]\n",
    "    targets = y_train.loc[random_point]\n",
    "    # Forward pass for the first layer\n",
    "    Z1 = np.dot(W1.T, input_data) + B1  # Linear transformation\n",
    "    A1 = sigmoid(Z1)  # Sigmoid activation\n",
    "    #forward pass output layer\n",
    "    Z2 = np.dot(W2.T, A1) + B2  # Linear transformation\n",
    "    Y = sigmoid(Z2)  # Sigmoid activation\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the cross-entropy loss\n",
    "    loss = Loss(targets, Y)\n",
    "\n",
    "    # Backpropagation\n",
    "\n",
    "    # Calculate the derivative of the loss with respect to the output\n",
    "    dL_dY = Loss_derivative(targets, Y)\n",
    "\n",
    "    # Calculate the derivative of the output layer\n",
    "    dY_dZ2 = sigmoid_deriv(Z2)\n",
    "    dZ2_dW2 = A1\n",
    "\n",
    "    # gradients for weights and biases in output layer\n",
    "    dL_dW2 = np.outer(dZ2_dW2, dL_dY * dY_dZ2)\n",
    "    dL_dB2 = dL_dY * dY_dZ2\n",
    "\n",
    "    # derivative of the hidden layer\n",
    "    dZ2_dA1 = W2\n",
    "    dA1_dZ1 = sigmoid_deriv(Z1)\n",
    "    dZ1_dW1 = input_data\n",
    "\n",
    "    # Calculate the gradients for weights and biases in the hidden layer @ is matrix multip.\n",
    "    dL_dW1 = np.outer(dZ1_dW1, (dL_dY * dY_dZ2) @ dZ2_dA1.T * dA1_dZ1)\n",
    "    dL_dB1 = (dL_dY * dY_dZ2) @ dZ2_dA1.T * dA1_dZ1\n",
    "\n",
    "    # Update the weights and biases in the output layer\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    B2 -= learning_rate * dL_dB2\n",
    "\n",
    "    # Update the weights and biases in the hidden layer\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    B1 -= learning_rate * dL_dB1\n",
    "\n",
    "    print(f\"Iteration {i}: \\n Loss: {np.mean(loss)}\")\n",
    "    #print(f\"Iteration {i}: \\n Loss: {loss}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
