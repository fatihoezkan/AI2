{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "##################################\n",
    "# For matrices or arbitrary size #\n",
    "##################################\n",
    "class MyWeightTensor:\n",
    "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
    "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
    "        self.shape = shape\n",
    "\n",
    "        if init_weights is not None:\n",
    "            if isinstance(init_weights, MyWeightTensor):\n",
    "                self.values = init_weights.values\n",
    "            else:\n",
    "                if isinstance(shape, tuple):\n",
    "                    assert isinstance(init_weights, np.ndarray)\n",
    "                else:\n",
    "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
    "                \n",
    "                self.values = init_weights\n",
    "        else:\n",
    "            if isinstance(shape, int):\n",
    "                self.shape = (self.shape,)\n",
    "                self.values = init_weight_fn(shape)\n",
    "            else:\n",
    "                self.values = init_weight_fn(*shape)\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'MyWeightTensor':\n",
    "        _T = self.values.T\n",
    "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
    "    \n",
    "    def __add__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
    "\n",
    "    def __mul__(self, other) -> 'MyWeightTensor':\n",
    "        if isinstance(other, MyWeightTensor):\n",
    "            other = other.values\n",
    "        else:\n",
    "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
    "        \n",
    "        _dot = np.dot(self.values, other)\n",
    "\n",
    "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)\n",
    "\n",
    "\n",
    "###############################\n",
    "# For creating a linear layer #\n",
    "###############################\n",
    "class MyLinearLayer:\n",
    "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
    "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
    "\n",
    "        self.latest_input = None\n",
    "        self.latest_output = None\n",
    "\n",
    "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
    "        self.latest_input = tensor\n",
    "\n",
    "        bs = -1\n",
    "        if len(tensor.shape) == 2:\n",
    "            # batch size included\n",
    "            bs = tensor.shape[0]\n",
    "            _w = self.weights * tensor.T\n",
    "        else:\n",
    "            _w = self.weights * tensor\n",
    "        \n",
    "        _bias = self.bias.values\n",
    "        if bs != -1:\n",
    "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
    "        \n",
    "        self.latest_output = (_w + _bias.T).T\n",
    "\n",
    "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
    "    \n",
    "    def derivative(self) -> float:\n",
    "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
    "        # Linear activation derivation\n",
    "        return np.ones(shape=self.latest_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Creating a custom neural network #\n",
    "####################################\n",
    "\n",
    "def xavier_normal_init(*shape) -> np.ndarray:\n",
    "    assert len(shape) <= 2, 'Can only init max 2d tensors'\n",
    "    fan_in = shape[0]\n",
    "    if len(shape) == 1:\n",
    "        fan_out = fan_in\n",
    "    else:\n",
    "        fan_out = shape[1]\n",
    "    gain = 1.0\n",
    "\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(loc=0.0, scale=std, size=shape)\n",
    "\n",
    "\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self) -> None:\n",
    "        # init_weight_fn = lambda *shape: np.random.randn(*shape) / 10\n",
    "        init_weight_fn = lambda *shape: xavier_normal_init(*shape)\n",
    "        self.layers = [\n",
    "            MyLinearLayer(in_features=784, out_features=32, init_weight_fn=init_weight_fn),\n",
    "            MyLinearLayer(in_features=32, out_features=32, init_weight_fn=init_weight_fn),\n",
    "            MyLinearLayer(in_features=32, out_features=10, init_weight_fn=init_weight_fn)\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, tensor: np.ndarray) -> Any:\n",
    "        x = tensor\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softmax(input: np.ndarray) -> np.ndarray:\n",
    "    _softmax = np.asarray([np.exp(_in) /np.sum(np.exp(_in), axis=0) for _in in input])\n",
    "\n",
    "    return _softmax\n",
    "\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, predictions: MyWeightTensor or np.ndarray, targets: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes cross entropy between targets snd predictions.    \n",
    "        Returns: List of cross entropy losses (batch-wise)\n",
    "        \"\"\"\n",
    "        if isinstance(predictions, MyWeightTensor):\n",
    "            predictions = predictions.values\n",
    "        \n",
    "        if isinstance(targets, MyWeightTensor):\n",
    "            targets = targets.values\n",
    "\n",
    "        assert predictions.shape[0] == targets.shape[0]\n",
    "        if len(targets.shape) == 2:\n",
    "            targets = targets.reshape(-1)\n",
    "        predictions = torch.as_tensor(predictions)\n",
    "        targets = torch.as_tensor(targets)\n",
    "\n",
    "        loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(predictions, targets)])\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def derivative(self) -> Callable:\n",
    "        # y_hat is the prediction\n",
    "        # y is the target value\n",
    "        def _derivative(y_hat: MyWeightTensor or np.ndarray, y: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
    "            if isinstance(y_hat, MyWeightTensor):\n",
    "                y_hat = y_hat.values\n",
    "            \n",
    "            if isinstance(y, MyWeightTensor):\n",
    "                y = y.values\n",
    "\n",
    "            _y = np.zeros(shape=y_hat.shape)\n",
    "            np.put_along_axis(_y, y, 1, axis=-1)\n",
    "\n",
    "            y_hat = softmax(y_hat)\n",
    "\n",
    "            return y_hat - _y\n",
    "        \n",
    "        return _derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: MyNeuralNetwork, batch_size: int, learning_rate: float, loss_fn: Callable, epochs: int = 10):\n",
    "    train_loader, _ = load_mnist_data(batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "\n",
    "            # for custom model\n",
    "            imgs = imgs.numpy()\n",
    "            targets = targets.numpy()\n",
    "\n",
    "            if len(targets.shape) == 1:\n",
    "                targets = targets.reshape(-1, 1)\n",
    "\n",
    "            imgs = imgs.reshape(-1, 28 * 28)\n",
    "\n",
    "            imgs = MyWeightTensor(shape=imgs.shape, init_weights=imgs)\n",
    "\n",
    "            outputs = model(imgs).values\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            avg_loss = np.mean(loss)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += avg_loss\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = np.argmax(outputs, axis=1)\n",
    "            accuracy = (max_outputs == targets.flatten()).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "\n",
    "            #########################\n",
    "            # Start backpropagation #\n",
    "            #########################\n",
    "\n",
    "            # starting from the very end with loss function\n",
    "            loss_derivative = loss_fn.derivative()\n",
    "            loss_grad = loss_derivative(outputs, targets)\n",
    "\n",
    "            # latest grads contains the gradient of the layer nodes (output node, e. g. after activation function)\n",
    "            # beginning with gradients of the loss\n",
    "            layer_out_grads = loss_grad\n",
    "            # starting from the last layer\n",
    "            for layer in model.layers[::-1]:\n",
    "                layer_activation_derivative = layer.derivative()\n",
    "\n",
    "                # layer activation (hadamard prod)\n",
    "                w_grad = layer_activation_derivative * layer_out_grads\n",
    "\n",
    "                # inputs of the prev layer are the derivatives of the weight matrix\n",
    "                w_grad = np.dot(w_grad.T, layer.latest_input.values)\n",
    "\n",
    "                # inputs of bias edges (weights) are always 1\n",
    "                b_grad = np.mean(layer_out_grads, axis=0)\n",
    "\n",
    "                # new output gradients (for previous layer calculations)\n",
    "                layer_out_grads = np.dot(layer_out_grads, layer.weights.values)  # / batch_size\n",
    "\n",
    "                # update weights of the current layer\n",
    "                layer.weights.values = layer.weights.values - learning_rate * (w_grad / batch_size)\n",
    "                layer.bias.values = layer.bias.values - learning_rate * (b_grad / batch_size)\n",
    "\n",
    "            #######################\n",
    "            # End backpropagation #\n",
    "            #######################\n",
    "\n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy: {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1: 100%|██████████| 15000/15000 [01:02<00:00, 240.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with loss: 0.480 and accuracy: 0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 2: 100%|██████████| 15000/15000 [01:00<00:00, 249.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with loss: 0.330 and accuracy: 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 3: 100%|██████████| 15000/15000 [01:03<00:00, 237.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished with loss: 0.312 and accuracy: 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 4: 100%|██████████| 15000/15000 [01:01<00:00, 244.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished with loss: 0.303 and accuracy: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 5: 100%|██████████| 15000/15000 [01:01<00:00, 244.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished with loss: 0.296 and accuracy: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 6:  30%|███       | 4505/15000 [00:30<01:12, 145.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      8\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, batch_size, learning_rate, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m imgs \u001b[38;5;241m=\u001b[39m MyWeightTensor(shape\u001b[38;5;241m=\u001b[39mimgs\u001b[38;5;241m.\u001b[39mshape, init_weights\u001b[38;5;241m=\u001b[39mimgs)\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(imgs)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(loss)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__call__\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     27\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(predictions)\n\u001b[0;32m---> 29\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m(targets)\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([F\u001b[38;5;241m.\u001b[39mcross_entropy(pred, t)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m pred, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, targets)])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Execute the training loop #\n",
    "#############################\n",
    "model = MyNeuralNetwork()\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    loss_fn=loss_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('aai2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9621b288640ec72265c2f0c931c1129106da1ad09ceb1028ae8cec1cf5ff8f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
